{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JensH-2157843/AML_Project/blob/main/src/neural_networks/NN1(segmentation).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library imports"
      ],
      "metadata": {
        "id": "Um0i7fD1wrYv"
      },
      "id": "Um0i7fD1wrYv"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models==1.0.1 albumentations==1.3.1 --quiet\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import SegformerFeatureExtractor\n",
        "\n",
        "import time\n",
        "import copy\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NABThOXpwtiG",
        "outputId": "aea95c38-8808-42c1-ad9b-e032ba4b6a0b"
      },
      "id": "NABThOXpwtiG",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset import"
      ],
      "metadata": {
        "id": "903c_9H44Wfs"
      },
      "id": "903c_9H44Wfs"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LlRduzl82rms",
        "outputId": "fcf30d97-0b9a-4c37-a849-9ed70a276737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LlRduzl82rms",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DATASET IMPORT ##\n",
        "deepglobe_dir = \"/content/drive/MyDrive/train\"\n",
        "import os\n",
        "\n",
        "deepglobe_images = sorted(glob(os.path.join(deepglobe_dir, '*_sat.jpg')))\n",
        "deepglobe_masks = sorted(glob(os.path.join(deepglobe_dir, '*_mask.png')))\n",
        "\n",
        "for tile in sorted(os.listdir(deepglobe_dir)):\n",
        "    tile_path = os.path.join(deepglobe_dir, tile)\n",
        "    if not os.path.isdir(tile_path):\n",
        "        continue\n",
        "    img_folder = os.path.join(tile_path, \"images\")\n",
        "    mask_folder = os.path.join(tile_path, \"masks\")\n",
        "    deepglobe_images.extend(sorted(glob(os.path.join(img_folder, '*.jpg'))))\n",
        "    deepglobe_masks.extend(sorted(glob(os.path.join(mask_folder, '*.png'))))\n",
        "\n",
        "all_images = deepglobe_images\n",
        "all_masks = deepglobe_masks\n",
        "\n",
        "train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n",
        "    all_images, all_masks, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "LA8Iaee_wwtJ"
      },
      "id": "LA8Iaee_wwtJ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (256, 256)\n",
        "\n",
        "def rgb_to_binary_mask(mask_image, suitable_rgbs):\n",
        "    mask = np.array(mask_image)\n",
        "    binary_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)\n",
        "    for rgb in suitable_rgbs:\n",
        "        matches = np.all(mask == rgb, axis=-1)\n",
        "        binary_mask[matches] = 1\n",
        "    return binary_mask\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "mask_transforms = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.NEAREST)\n",
        "])"
      ],
      "metadata": {
        "id": "71Pb4N7C6fu7"
      },
      "id": "71Pb4N7C6fu7",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Your PyTorch Dataset (This IS your combined loader + preprocessor) ---\n",
        "class SolarPanelDataset(Dataset):\n",
        "    def __init__(self, img_paths, mask_paths, suitable_rgbs, img_transform=None, mask_transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.suitable_rgbs = suitable_rgbs\n",
        "        self.img_transform = img_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # --- Start of Preprocessing Logic (PyTorch version) ---\n",
        "        img_path = self.img_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask_rgb = Image.open(mask_path).convert(\"RGB\")\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image) # Applies resize, ToTensor, Normalize\n",
        "        if self.mask_transform:\n",
        "            mask_rgb = self.mask_transform(mask_rgb) # Applies resize (NEAREST)\n",
        "        mask_binary = rgb_to_binary_mask(mask_rgb, self.suitable_rgbs) # Converts mask\n",
        "        mask = torch.from_numpy(mask_binary) # To PyTorch Tensor\n",
        "        # --- End of Preprocessing Logic ---\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "EB3qzu3R6j4O"
      },
      "id": "EB3qzu3R6j4O",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RGB values for classes we consider 'Suitable' (Class 1)\n",
        "SUITABLE_RGB_VALUES = [\n",
        "    (255, 255, 0),  # Agriculture land\n",
        "    (255, 0, 255),  # Rangeland\n",
        "    (255, 255, 255),# Barren land\n",
        "    (60, 16, 152),  # Building\n",
        "    (132, 41, 246)  # Unpaved land\n",
        "]"
      ],
      "metadata": {
        "id": "Or0VPK0E7bjU"
      },
      "id": "Or0VPK0E7bjU",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = SolarPanelDataset(train_imgs, train_masks, SUITABLE_RGB_VALUES,  image_transforms, mask_transforms)\n",
        "train_loader = SolarPanelDataset(val_imgs, val_masks, SUITABLE_RGB_VALUES,  image_transforms, mask_transforms)\n",
        "\n",
        "val_loader = DataLoader(val_loader, batch_size=20, shuffle=False)\n",
        "train_loader = DataLoader(train_loader, batch_size=20, shuffle=True)"
      ],
      "metadata": {
        "id": "c9LRSwLP7Izf"
      },
      "id": "c9LRSwLP7Izf",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "90aqShfs7QBS"
      },
      "id": "90aqShfs7QBS"
    },
    {
      "cell_type": "code",
      "source": [
        "## ARCHITECTURE ##\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Block: (Conv -> BN -> ReLU) * 2\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder Block: ConvBlock -> MaxPool\n",
        "    Returns both ConvBlock output (skip) and MaxPool output.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv_block(x)\n",
        "        pooled = self.pool(skip)\n",
        "        return skip, pooled\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder Block: ConvTranspose -> Concat -> ConvBlock\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        # Upsamples by a factor of 2, halving the channels.\n",
        "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        # ConvBlock takes concatenated input (skip + upconv), so its input channels\n",
        "        # will be out_channels (from skip) + out_channels (from upconv).\n",
        "        self.conv_block = ConvBlock(out_channels * 2, out_channels)\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        x = self.upconv(x)\n",
        "\n",
        "        # Ensure spatial dimensions match before concatenating.\n",
        "        # If input sizes are powers of 2, they should match.\n",
        "        # If not, cropping (from skip) or padding (to x) might be needed.\n",
        "        # Here we assume they match or crop the skip connection if necessary.\n",
        "        if x.shape != skip_connection.shape:\n",
        "            # Simple center-cropping (adjust if needed)\n",
        "            diffY = skip_connection.size()[2] - x.size()[2]\n",
        "            diffX = skip_connection.size()[3] - x.size()[3]\n",
        "            skip_connection = skip_connection[:, :, diffY // 2 : skip_connection.size()[2] - diffY // 2 - diffY % 2,\n",
        "                                                diffX // 2 : skip_connection.size()[3] - diffX // 2 - diffX % 2]\n",
        "\n",
        "        x = torch.cat([x, skip_connection], dim=1) # Concatenate along channel dimension (dim=1)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "class DeepUnet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_classes=11):\n",
        "        \"\"\"\n",
        "        Initializes the DeepUnet model.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
        "            out_classes (int): Number of output segmentation classes.\n",
        "        \"\"\"\n",
        "        super(DeepUnet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_classes = out_classes\n",
        "\n",
        "        # Encoder Path\n",
        "        self.enc1 = EncoderBlock(in_channels, 64)\n",
        "        self.enc2 = EncoderBlock(64, 128)\n",
        "        self.enc3 = EncoderBlock(128, 256)\n",
        "        self.enc4 = EncoderBlock(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = ConvBlock(512, 1024)\n",
        "\n",
        "        # Decoder Path\n",
        "        self.dec1 = DecoderBlock(1024, 512)\n",
        "        self.dec2 = DecoderBlock(512, 256)\n",
        "        self.dec3 = DecoderBlock(256, 128)\n",
        "        self.dec4 = DecoderBlock(128, 64)\n",
        "\n",
        "        # Output Layer\n",
        "        self.output_conv = nn.Conv2d(64, out_classes, kernel_size=1)\n",
        "\n",
        "        # Optional: Softmax layer. Often omitted if using CrossEntropyLoss,\n",
        "        # which combines LogSoftmax and NLLLoss.\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the U-Net.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor (N, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output segmentation map (N, out_classes, H, W).\n",
        "        \"\"\"\n",
        "        # Encoder path\n",
        "        s1, p1 = self.enc1(x)\n",
        "        s2, p2 = self.enc2(p1)\n",
        "        s3, p3 = self.enc3(p2)\n",
        "        s4, p4 = self.enc4(p3)\n",
        "\n",
        "        # Bottleneck\n",
        "        b1 = self.bottleneck(p4)\n",
        "\n",
        "        # Decoder path\n",
        "        d1 = self.dec1(b1, s4)\n",
        "        d2 = self.dec2(d1, s3)\n",
        "        d3 = self.dec3(d2, s2)\n",
        "        d4 = self.dec4(d3, s1)\n",
        "\n",
        "        # Output\n",
        "        outputs = self.output_conv(d4)\n",
        "\n",
        "        # Optional: Apply softmax\n",
        "        # outputs = self.softmax(outputs)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "QJ3phlnJ9iHR"
      },
      "id": "QJ3phlnJ9iHR",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning algorithm"
      ],
      "metadata": {
        "id": "snPbqh5A5MIg"
      },
      "id": "snPbqh5A5MIg"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration & Constants ---\n",
        "IMG_SIZE = (256, 256)\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 50 # A good starting point, adjust as needed\n",
        "IN_CHANNELS = 3\n",
        "OUT_CLASSES = 2 # 0: Not Suitable, 1: Suitable\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# EarlyStopping Configuration\n",
        "EARLY_STOPPING_PATIENCE = 7 # Number of epochs to wait for improvement before stopping\n",
        "EARLY_STOPPING_MIN_DELTA = 0.0001 # Minimum change in monitored quantity to qualify as improvement\n",
        "\n",
        "# --- 4. Model, Loss, Optimizer ---\n",
        "print(\"Setting up model, loss, and optimizer...\")\n",
        "model = DeepUnet(in_channels=IN_CHANNELS, out_classes=OUT_CLASSES).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# --- 5. Training Loop with EarlyStopping and History ---\n",
        "print(\"Starting training...\")\n",
        "history = {'train_loss': [], 'val_loss': []}\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "best_model_weights = copy.deepcopy(model.state_dict()) # Store best model\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    for i, (images, masks) in enumerate(train_loader):\n",
        "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_train_loss += loss.item()\n",
        "        if (i + 1) % 20 == 0: # Print training progress more frequently\n",
        "             print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i+1}/{len(train_loader)}], Batch Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            running_val_loss += loss.item()\n",
        "    avg_val_loss = running_val_loss / len(val_loader)\n",
        "    history['val_loss'].append(avg_val_loss)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} Finished ---\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Epoch Duration: {epoch_time:.2f} seconds\")\n",
        "\n",
        "    # EarlyStopping Check\n",
        "    if avg_val_loss < best_val_loss - EARLY_STOPPING_MIN_DELTA:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        best_model_weights = copy.deepcopy(model.state_dict())\n",
        "        print(f\"Validation loss improved. Saving model weights.\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "    if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
        "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "        model.load_state_dict(best_model_weights) # Restore best model weights\n",
        "        break\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"Training finished!\")\n",
        "if epoch < NUM_EPOCHS -1 and epochs_no_improve < EARLY_STOPPING_PATIENCE : # If not early stopped\n",
        "    print(\"Completed all epochs.\")\n",
        "    model.load_state_dict(best_model_weights) # Ensure best model is loaded if early stopping wasn't triggered but patience was > 0\n",
        "\n",
        "# --- 6. Print Loss History ---\n",
        "print(\"\\n--- Training History ---\")\n",
        "for i in range(len(history['train_loss'])):\n",
        "    print(f\"Epoch {i+1}: Train Loss = {history['train_loss'][i]:.4f}, Val Loss = {history['val_loss'][i]:.4f}\")"
      ],
      "metadata": {
        "id": "a3IlxdCJ5S40",
        "outputId": "3a297d77-9221-46b7-9b73-7d2ac2026e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "a3IlxdCJ5S40",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up model, loss, and optimizer...\n",
            "Starting training...\n",
            "--- Epoch 1/50 Finished ---\n",
            "Train Loss: 0.7818 | Val Loss: 0.6749\n",
            "Epoch Duration: 613.67 seconds\n",
            "Validation loss improved. Saving model weights.\n",
            "------------------------------\n",
            "--- Epoch 2/50 Finished ---\n",
            "Train Loss: 0.6248 | Val Loss: 0.6331\n",
            "Epoch Duration: 179.33 seconds\n",
            "Validation loss improved. Saving model weights.\n",
            "------------------------------\n",
            "--- Epoch 3/50 Finished ---\n",
            "Train Loss: 0.5680 | Val Loss: 0.5611\n",
            "Epoch Duration: 179.05 seconds\n",
            "Validation loss improved. Saving model weights.\n",
            "------------------------------\n",
            "--- Epoch 4/50 Finished ---\n",
            "Train Loss: 0.5083 | Val Loss: 0.5130\n",
            "Epoch Duration: 174.86 seconds\n",
            "Validation loss improved. Saving model weights.\n",
            "------------------------------\n",
            "--- Epoch 5/50 Finished ---\n",
            "Train Loss: 0.5259 | Val Loss: 0.4906\n",
            "Epoch Duration: 177.66 seconds\n",
            "Validation loss improved. Saving model weights.\n",
            "------------------------------\n",
            "--- Epoch 6/50 Finished ---\n",
            "Train Loss: 0.4511 | Val Loss: 0.4495\n",
            "Epoch Duration: 176.89 seconds\n",
            "Validation loss improved. Saving model weights.\n",
            "------------------------------\n",
            "--- Epoch 7/50 Finished ---\n",
            "Train Loss: 0.4638 | Val Loss: 0.4702\n",
            "Epoch Duration: 177.46 seconds\n",
            "Validation loss did not improve for 1 epoch(s).\n",
            "------------------------------\n",
            "--- Epoch 8/50 Finished ---\n",
            "Train Loss: 0.4897 | Val Loss: 0.4384\n",
            "Epoch Duration: 179.51 seconds\n",
            "Validation loss improved. Saving model weights.\n",
            "------------------------------\n",
            "--- Epoch 9/50 Finished ---\n",
            "Train Loss: 0.4659 | Val Loss: 0.4354\n",
            "Epoch Duration: 171.31 seconds\n",
            "Validation loss improved. Saving model weights.\n",
            "------------------------------\n",
            "--- Epoch 10/50 Finished ---\n",
            "Train Loss: 0.4697 | Val Loss: 0.4745\n",
            "Epoch Duration: 177.05 seconds\n",
            "Validation loss did not improve for 1 epoch(s).\n",
            "------------------------------\n",
            "--- Epoch 11/50 Finished ---\n",
            "Train Loss: 0.4557 | Val Loss: 0.4429\n",
            "Epoch Duration: 177.33 seconds\n",
            "Validation loss did not improve for 2 epoch(s).\n",
            "------------------------------\n",
            "--- Epoch 12/50 Finished ---\n",
            "Train Loss: 0.4404 | Val Loss: 0.4716\n",
            "Epoch Duration: 177.85 seconds\n",
            "Validation loss did not improve for 3 epoch(s).\n",
            "------------------------------\n",
            "--- Epoch 13/50 Finished ---\n",
            "Train Loss: 0.4003 | Val Loss: 1.0669\n",
            "Epoch Duration: 178.50 seconds\n",
            "Validation loss did not improve for 4 epoch(s).\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path where you want to save the model\n",
        "model_save_path = \"solar_unet_model.pth\"\n",
        "\n",
        "# Save only the model's state dictionary (recommended for inference/retraining)\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Neural network model saved successfully to {model_save_path}\")"
      ],
      "metadata": {
        "id": "vwfQn9p1HNBc"
      },
      "id": "vwfQn9p1HNBc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}