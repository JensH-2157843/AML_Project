{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JensH-2157843/AML_Project/blob/main/src/neural_networks/NN1(segmentation).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library imports"
      ],
      "metadata": {
        "id": "Um0i7fD1wrYv"
      },
      "id": "Um0i7fD1wrYv"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models==1.0.1 albumentations==1.3.1 --quiet\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import SegformerFeatureExtractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NABThOXpwtiG",
        "outputId": "491956ad-d176-4993-c155-0d1f590a1bc5"
      },
      "id": "NABThOXpwtiG",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset import"
      ],
      "metadata": {
        "id": "903c_9H44Wfs"
      },
      "id": "903c_9H44Wfs"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LlRduzl82rms"
      },
      "id": "LlRduzl82rms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DATASET IMPORT ##\n",
        "deepglobe_dir = \"/content/drive/MyDrive/train\"\n",
        "import os\n",
        "\n",
        "deepglobe_images = sorted(glob(os.path.join(deepglobe_dir, '*_sat.jpg')))\n",
        "deepglobe_masks = sorted(glob(os.path.join(deepglobe_dir, '*_mask.png')))\n",
        "\n",
        "for tile in sorted(os.listdir(deepglobe_dir)):\n",
        "    tile_path = os.path.join(deepglobe_dir, tile)\n",
        "    if not os.path.isdir(tile_path):\n",
        "        continue\n",
        "    img_folder = os.path.join(tile_path, \"images\")\n",
        "    mask_folder = os.path.join(tile_path, \"masks\")\n",
        "    deepglobe_images.extend(sorted(glob(os.path.join(img_folder, '*.jpg'))))\n",
        "    deepglobe_masks.extend(sorted(glob(os.path.join(mask_folder, '*.png'))))\n",
        "\n",
        "all_images = deepglobe_images\n",
        "all_masks = deepglobe_masks\n",
        "\n",
        "train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n",
        "    all_images, all_masks, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA8Iaee_wwtJ",
        "outputId": "51e646ad-21f1-4514-da54-c5ba094b70f7"
      },
      "id": "LA8Iaee_wwtJ",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "90aqShfs7QBS"
      },
      "id": "90aqShfs7QBS"
    },
    {
      "cell_type": "code",
      "source": [
        "## ARCHITECTURE ##\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Block: (Conv -> BN -> ReLU) * 2\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder Block: ConvBlock -> MaxPool\n",
        "    Returns both ConvBlock output (skip) and MaxPool output.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv_block(x)\n",
        "        pooled = self.pool(skip)\n",
        "        return skip, pooled\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder Block: ConvTranspose -> Concat -> ConvBlock\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        # Upsamples by a factor of 2, halving the channels.\n",
        "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        # ConvBlock takes concatenated input (skip + upconv), so its input channels\n",
        "        # will be out_channels (from skip) + out_channels (from upconv).\n",
        "        self.conv_block = ConvBlock(out_channels * 2, out_channels)\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        x = self.upconv(x)\n",
        "\n",
        "        # Ensure spatial dimensions match before concatenating.\n",
        "        # If input sizes are powers of 2, they should match.\n",
        "        # If not, cropping (from skip) or padding (to x) might be needed.\n",
        "        # Here we assume they match or crop the skip connection if necessary.\n",
        "        if x.shape != skip_connection.shape:\n",
        "            # Simple center-cropping (adjust if needed)\n",
        "            diffY = skip_connection.size()[2] - x.size()[2]\n",
        "            diffX = skip_connection.size()[3] - x.size()[3]\n",
        "            skip_connection = skip_connection[:, :, diffY // 2 : skip_connection.size()[2] - diffY // 2 - diffY % 2,\n",
        "                                                diffX // 2 : skip_connection.size()[3] - diffX // 2 - diffX % 2]\n",
        "\n",
        "        x = torch.cat([x, skip_connection], dim=1) # Concatenate along channel dimension (dim=1)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "class DeepUnet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_classes=11):\n",
        "        \"\"\"\n",
        "        Initializes the DeepUnet model.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
        "            out_classes (int): Number of output segmentation classes.\n",
        "        \"\"\"\n",
        "        super(DeepUnet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_classes = out_classes\n",
        "\n",
        "        # Encoder Path\n",
        "        self.enc1 = EncoderBlock(in_channels, 64)\n",
        "        self.enc2 = EncoderBlock(64, 128)\n",
        "        self.enc3 = EncoderBlock(128, 256)\n",
        "        self.enc4 = EncoderBlock(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = ConvBlock(512, 1024)\n",
        "\n",
        "        # Decoder Path\n",
        "        self.dec1 = DecoderBlock(1024, 512)\n",
        "        self.dec2 = DecoderBlock(512, 256)\n",
        "        self.dec3 = DecoderBlock(256, 128)\n",
        "        self.dec4 = DecoderBlock(128, 64)\n",
        "\n",
        "        # Output Layer\n",
        "        self.output_conv = nn.Conv2d(64, out_classes, kernel_size=1)\n",
        "\n",
        "        # Optional: Softmax layer. Often omitted if using CrossEntropyLoss,\n",
        "        # which combines LogSoftmax and NLLLoss.\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the U-Net.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor (N, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output segmentation map (N, out_classes, H, W).\n",
        "        \"\"\"\n",
        "        # Encoder path\n",
        "        s1, p1 = self.enc1(x)\n",
        "        s2, p2 = self.enc2(p1)\n",
        "        s3, p3 = self.enc3(p2)\n",
        "        s4, p4 = self.enc4(p3)\n",
        "\n",
        "        # Bottleneck\n",
        "        b1 = self.bottleneck(p4)\n",
        "\n",
        "        # Decoder path\n",
        "        d1 = self.dec1(b1, s4)\n",
        "        d2 = self.dec2(d1, s3)\n",
        "        d3 = self.dec3(d2, s2)\n",
        "        d4 = self.dec4(d3, s1)\n",
        "\n",
        "        # Output\n",
        "        outputs = self.output_conv(d4)\n",
        "\n",
        "        # Optional: Apply softmax\n",
        "        # outputs = self.softmax(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Example of how to create and use the model:\n",
        "if __name__ == '__main__':\n",
        "    # Ensure input dimensions are powers of 2 for easy up/down sampling\n",
        "    input_tensor = torch.randn(1, 3, 256, 256) # (N, C, H, W)\n",
        "    num_classes = 11\n",
        "\n",
        "    # Create an instance of the model\n",
        "    model = DeepUnet(in_channels=3, out_classes=num_classes)\n",
        "\n",
        "    # Perform a forward pass\n",
        "    output = model(input_tensor)\n",
        "\n",
        "    # Print input and output shapes\n",
        "    print(f\"Input Tensor Shape: {input_tensor.shape}\")\n",
        "    print(f\"Output Tensor Shape: {output.shape}\")\n",
        "\n",
        "    # You can print the model structure (optional)\n",
        "    # print(model)\n",
        "\n",
        "    # Calculate number of parameters (optional)\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Number of trainable parameters: {num_params:,}\")"
      ],
      "metadata": {
        "id": "QJ3phlnJ9iHR",
        "outputId": "26b9552f-3b4f-4e6e-ccbc-7652eaf47611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QJ3phlnJ9iHR",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Tensor Shape: torch.Size([1, 3, 256, 256])\n",
            "Output Tensor Shape: torch.Size([1, 11, 256, 256])\n",
            "Number of trainable parameters: 31,038,283\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}